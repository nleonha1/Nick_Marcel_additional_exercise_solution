{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Exercise\n",
    "\n",
    "Due: **12:00 18 July 2022**\n",
    "\n",
    "**Online submission** at via [ILIAS](https://www.ilias.uni-koeln.de/ilias/goto_uk_exc_4593683.html) in the directory Exercises / Übungen -> Submission of Exercises / Rückgabe des Übungsblätter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marcel Günther 7348156, Nick Leonhardt 7348384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This additional exercise may be used for **one** of two reasons:\n",
    "\n",
    "* make up for the attendance requirement (if you missed more than two tutorials), or\n",
    "* replace lowest grade in exercises (mentioned to you via email)\n",
    "\n",
    "For the latter, you may submit the solutions to this like any other exercise this year. For experience, it is advantageous to still use the following procedure.\n",
    "\n",
    "If you need to make up for the attendance requirement, you must use a specific method of submission utilising [github](https://github.com/) and a makefile. If you are using Windows, please ensure you work either in [cygwin](https://www.cygwin.com/) or [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/install), since `make` does not work in Windows.\n",
    "\n",
    "1. create a repository on github exclusively for this exercise\n",
    "2. fully complete (or at least attempt) each question in a jupyter notebook\n",
    "   * name it `[name]_additional_exercise_solution.ipynb`\n",
    "3. restart and clear all notebook ouputs so it contains just the markdown and code cells (no plots or other outputs)\n",
    "4. create a makefile to execute the notebook, compile to a pdf, and remove any temporary files\n",
    "   * I should only need to run `$make` in order to build your submission\n",
    "   * **do not** submit a compiled pdf of the solution\n",
    "5. submit a file `[name].txt` containing the link to your git repository\n",
    "\n",
    "Although make is a rather archaic tool, `make` is very useful to perform low-level compilation. It actually began as an easier method to compile C** and fortran code. Common compilation routines are identified using *suffix rules*. **For an additional 10%, write your own suffix rule to compile `.ipynb` files into `.pdf`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MakeFile`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a makefile does is simplify a workflow using terminal commands. As mentioned above, it is useful to compile documentation/code if changes where made in the source. One can in fact use this for newer formats such as IPython notebooks (`.ipynb` files).\n",
    "\n",
    "A makefile typically goes in the top directory of your project. You can find an introduction to makefiles [here](https://makefiletutorial.com/). Basically how a makefile works is that you have a series of *targets* that execute a specific set of commands. For example, if your makefile contains,\n",
    "\n",
    "```\n",
    "info:\n",
    "    @echo \"This is a makefile.\"\n",
    "\n",
    "command:\n",
    "    @echo \"This is a command.\"\n",
    "```\n",
    "\n",
    "> *Note:* Make will by default also print each command. Beginning the command with `@` suppresses this functionality.\n",
    "\n",
    "you can make either target `info` or target `command`. Running `make info` in the terminal from the top directory will print `This is a makefile.` to the terminal. Likewise running `make command` will print `This is a command.` to the terminal. Not specifying a target will make the first target in the file (so `make info` is the same as `make` in this example). For this reason, it is customary to set the first target as the default workflow. If our makefile now contains,\n",
    "\n",
    "```\n",
    "all: info command\n",
    "\n",
    "info:\n",
    "    @echo \"This is a makefile.\"\n",
    "\n",
    "command:\n",
    "    @echo \"Some commands to execute:\"\n",
    "    echo \"...\"\n",
    "    @echo \"It has now completed.\"\n",
    "```\n",
    "\n",
    "running `make` from the terminal will print,\n",
    "\n",
    "```\n",
    "This is a makefile.\n",
    "Some commands to execute:\n",
    "echo \"...\"\n",
    "...\n",
    "It has now completed.\n",
    "```\n",
    "\n",
    "Each target can run multiple commands, but each line is run in a separate subshell. This means if you want to string together commands (for example if you need to change directories), you have to write them in one line as `command 1 ; command 2 ; ...`.\n",
    "\n",
    "This should be most everything you need to write your own makefile. Please contact a TA if you run into any major issues.\n",
    "\n",
    "> Ensure each command is indented with a **tab** character (not *space* characters). This is one of the finicky aspects of using a makefile...\n",
    "\n",
    "> Note that linux operating systems use GNU make with `bash` or `dash` as a shell. Macintosh products typically use `zsh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a notebook from the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most operations you can perform on the notebook are available from the terminal. The command to work with notebooks is [nbconvert](https://nbconvert.readthedocs.io/en/latest/usage.html). This can convert between file types, depending on your needs. For example, executing a notebook from the terminal can be done using,\n",
    "\n",
    "```\n",
    "jupyter nbconvert --to notebook --execute notebook.ipynb\n",
    "```\n",
    "\n",
    "You can see a list of the available format types in the aforementioned link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import inf\n",
    "from scipy import interpolate\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the *Hipparcos* catalogue\n",
    "\n",
    "*Hipparcos* (HIgh Precision PARallax COllecting Satelite) was a very important stellar survey mission between 1989 and 1993. It was the first astrometric mission of its kind, and it's precise observations allowed for unprecedented astrometrical calculations within the Milky Way. The observations of faint stars have since been (*vastly*) improved upon by the current *Gaia* mission, though the brightest stars were still most accurately observed by *Hipparcos*.\n",
    "\n",
    "The following examples will utilise a subset of a more recent reduction of the *Hipparcos* [data](https://vizier.u-strasbg.fr/viz-bin/VizieR-3?-source=I/311/hip2) ([van Leeuwen 2007](https://arxiv.org/pdf/0708.1752.pdf)) to emphasize how work with astronomical data within python, and hopefully teach a thing or two about stellar populations. The dataset we use are the stars with five-parameter astrometric solutions, meaning the solutions have been constrained for RA and DEC, proper motion in RA and DEC, and parallax. We will also highlight some of the features of various python packages so you can decide which method you prefer.\n",
    "\n",
    "This dataset has 15 columns, with the titles as a comment in the first line. The columns (and units) are,\n",
    "\n",
    " - name in the *Hipparcos* catalog HIP\n",
    " - right ascentions RA ($\\mathrm{RA}$; degrees)\n",
    " - error in right ascention sigma_RA ($\\sigma_\\mathrm{RA}$; milli-arcseconds)\n",
    " - declination DEC ($\\mathrm{DEC}$; degrees)\n",
    " - error in declination sigma_DEC ($\\sigma_\\mathrm{DEC}$; milli-arcseconds)\n",
    " - parallax PLX ($\\varpi$; in milli-arcseconds)\n",
    " - error in parallax sigma_PLX ($\\sigma_\\mathrm{\\varpi}$; in milli-arcseconds)\n",
    " - right ascention proper motion PM_RA ($\\mu_\\mathrm{RA}$; milli-arcseconds per year)\n",
    " - error in right ascention proper motion sigma_PM_RA ($\\sigma_{\\mu, \\mathrm{RA}}$; milli-arcseconds per year)\n",
    " - declination proper motion PM_DEC ($\\mu_\\mathrm{DEC}$; milli-arcseconds per year)\n",
    " - error in declination proper motion sigma_PM_DEC ($\\sigma_{\\mu, \\mathrm{DEC}}$; milli-arcseconds per year)\n",
    " - *Hipparcos* magnitude Hp (dex)\n",
    "   - *aka* V-band magnitude\n",
    " - error in magnitude sigma_Hp (dex)\n",
    " - B-V color (dex)\n",
    " - error in B-V color sigma_B-V (dex)\n",
    " - V-I color (dex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major benefit to precise astrometric observations is an accurate estimate of the parallax. This allows us to calculate the distance to each star and it reveal the structure of the Milky Way.\n",
    "\n",
    "Begin by using the parallax ($\\varpi$) to calculate the distance in parsecs (pc) to each star. It is important to remember that the parallax has units of milli-arcseconds (mas), so the distance to the star is calculated by:\n",
    "\n",
    "$$ d = 1000 \\Big(\\frac{\\mathrm{mas}}{\\varpi}\\Big) \\ \\mathrm{pc} $$\n",
    "\n",
    "Use this formula to find some interesting trends in the *Hipparcos* catalog. Are there any issues with the dataset? **5 points**\n",
    "\n",
    "> *Note:* Each degree contains 60 arcminutes, which each contain 60 arcseconds, so there are 3 600 000 milli-arcseconds in 1 degree.\n",
    "\n",
    "> *Note:* The Milky Way has a diameter of $\\sim 40~000$ pc and Earth is located at a radius of $\\sim 8~000$ pc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find what is the most common distance to a star? What is the standard deviation of this value? What can you determine from these values? **5 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('hipparcos2.csv', sep=',')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_PLX = data['PLX']>data['sigma_PLX']\n",
    "filtered_PLX = filtering_PLX*data['PLX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_PLX.replace([np.inf, 0, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### issues with the data set is that there are some inf values and for a few values the error is bigger than the value itself. Therefore we are filtering the values where the error is bigger then the value it self and replace +-inf and 0 with nan. So these values will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Distance (pc)'] = (1000/filtered_PLX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = np.asarray(data['Distance (pc)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distance, bins=1000)\n",
    "plt.title('Distance of the stars in the Hipparcos catalog in pc ')\n",
    "plt.xlabel('Distance in pc')\n",
    "plt.ylabel('Quantity')\n",
    "plt.tight_layout()\n",
    "plt.axvline(x=100,color='red',linestyle='--')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the most common distance is around 100 pc')\n",
    "print('the mean distance is',data['Distance (pc)'].mean(skipna=True),'pc')\n",
    "print('the std of the distance is',data['Distance (pc)'].std(skipna=True),'pc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It can be can sayd that the most common distance is about 100pc, which also can be seen in the histogram above (red line). But there are huge differents between the distances which can be seen at the standard deviation value and also in the plot above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilising error\n",
    "\n",
    "Error is a major aspect of observational data, since there are so many sources (instrumentational and physical) to consider. Following the procedure of van Leeuwen (2007), let us examine the error in parallax and its trends in our dataset. Relative error $\\big(\\frac{\\sigma_q}{q}\\big)$ is an important metric when our data spans multiple orders of magnitude. What is the distribution of relative error in parallax (you may want to make y log-spaced)? What does this mean in terms of *Hipparcos*' precision? **10 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PLX'].replace([np.inf, 0, -np.inf], np.nan, inplace=True)\n",
    "data['sigma_PLX'].replace([np.inf, 0, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Relativ error PLX'] = abs(data['sigma_PLX']/data['PLX'])\n",
    "relativ_error= np.asarray(data['Relativ error PLX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(relativ_error,bins=np.arange(0,5,0.01))\n",
    "plt.title('Distribution of Relative Error of Parallax ')\n",
    "plt.xlabel('Relative Error')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Quantity in log scale')\n",
    "plt.tight_layout()\n",
    "plt.xlim((0,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By looking at plot above which is in y log scaled, it can be seen that there is still an log distribution. This means he distribution of relative error in parallax seems to be a double log distribution. In terms of Hipparcos' precision it can be said, that it is not good because too much values have a too high relative error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error in parallax is typically related to the brightness of the star (from the statistics of photon counts). A view of this trend can be seen below (keeping the relative parallax error below 5%; similar to Figure 11 in van Leewen 2007). Can you find a fit to this relation, including error? **10 points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![relative error](hipparcos_plx_err.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_error_filtering = data['Relativ error PLX'] <= 0.05 \n",
    "rel_error_filtered = rel_error_filtering* data['sigma_PLX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_error_filtered_np = np.asarray(rel_error_filtered)\n",
    "magnitude = np.asarray(data['Hp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=magnitude\n",
    "y=rel_error_filtered_np\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)\n",
    "plt.scatter(x,y,s=0.1)\n",
    "plt.yscale('log')\n",
    "plt.ylim((0.1,8))\n",
    "plt.xlim((1,13))\n",
    "plt.xlabel(\"HP\")\n",
    "plt.ylabel(\"sigma_PLX\")\n",
    "plt.gca().invert_xaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=magnitude\n",
    "y=rel_error_filtered_np\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(-x, y, 1))(np.unique(x)),color='red')\n",
    "plt.scatter(x,y,s=0.1)\n",
    "#plt.yscale('log')\n",
    "plt.ylim((0.1,8))\n",
    "plt.xlim((1,13))\n",
    "plt.xlabel(\"HP\")\n",
    "plt.ylabel(\"sigma_PLX\")\n",
    "plt.gca().invert_xaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we could only find a fit for the plot without ylog scaled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the fitted line has a and b of:',np.polyfit(-x, y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The residual value returned is the sum of the squares of the fit errors:', np.sum((np.polyval(np.polyfit(-x, y, 1), -x) - y)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above value tells us, that this fit is not good and there is a big deviation in the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you plot a histogram of the relative errors in $\\mathrm{RA}, \\ \\mathrm{DEC},$ and $\\mathrm{PLX}$ in a single figure? **10 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Relativer error RA'] = abs(data['sigma_RA']/data['RA'])\n",
    "RA = np.asarray(data['Relativer error RA'])\n",
    "\n",
    "data['Relativer error DEC'] = abs(data['sigma_DEC']/data['DEC'])\n",
    "DEC = np.asarray(data['Relativer error DEC'])\n",
    "\n",
    "\n",
    "data['Relativer error PLX'] = abs(data['sigma_PLX']/data['PLX'])\n",
    "data['Relativer error PLX'].replace([np.inf, 0, -np.inf], np.nan, inplace=True)\n",
    "PLX = np.asarray(data['Relativer error PLX'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(RA,bins=np.arange(0,0.5,0.01),label='RA',alpha=0.5)\n",
    "plt.hist(DEC,bins=np.arange(0,0.5,0.01),label='DEC',alpha=0.5)\n",
    "plt.hist(PLX,bins=np.arange(0,0.5,0.01),label='PLX',alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('relative errors')\n",
    "plt.ylabel('Quantity')\n",
    "plt.title('histogram of the relative errors in $\\mathrm{RA}, \\ \\mathrm{DEC},$ and $\\mathrm{PLX}$')\n",
    "plt.xlim((0,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By looking at the above histogram of the relative errors in $\\mathrm{RA}, \\ \\mathrm{DEC},$ and $\\mathrm{PLX}$, it can be said, that $\\mathrm{RA}$ has the best precission and $\\mathrm{PLX}$ the badest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hertzsprung-Russell diagrams\n",
    "\n",
    "#### Observational\n",
    "\n",
    "Before calculating any intrinsic stellar properties from the observations, it is typically useful to plot a diagram of what you *observe*. (Observed here means the most basic properties that can be determined from the data. If you want to be pedantic, then you might suggest that the values in this table are not what is actually observed, but a reduction of the observed photon counts.) For this purpose we should plot a Hertzsprung-Russel diagram (HRD) to reveal the different stellar types. It is important not only to remember that a HRD has colour (B-V) on the x-axis and magnitude (Hp) on the y-axis, but also that the y-axis is reversed (the perks of using an antiquated unit). **5 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.scatter(x = 'B-V',y = 'Hp',s=0.1)\n",
    "plt.title('HRD of Hippacros Catalogue')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It can be seen a lot of Values where B-V is null, and also a lot of points overlapping. It is like a blurred HRD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see?\n",
    "\n",
    "If you use the magnitude given in `hipparcos2.csv`, you might have a little difficulty determining the different regions of an HR diagram. One reason for this is that it is an apparent magnitude, so these results do not take into account reddening or extinction. Another issue, however, is that we don't filter the results that have a lot of error. Try plotting it again while limiting the relative parallax error to <20% and the color error to 5 mag. **5 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLX_error = data['sigma_PLX']/data['PLX']<0.2\n",
    "Color_error = data['sigma_B-V']<0.5\n",
    "\n",
    "PLX_Color_limit = PLX_error * Color_error\n",
    "\n",
    "B_V_limit = data['B-V']*PLX_Color_limit\n",
    "B_V_limit_np = np.asarray(B_V_limit)\n",
    "Hp_limit = data['Hp']*PLX_Color_limit\n",
    "Hp_limit_np = np.asarray(Hp_limit)\n",
    "V_I_limit = data['V-I']*PLX_Color_limit\n",
    "V_I_limit_np = np.asarray(V_I_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = B_V_limit_np, y = Hp_limit_np, s = 0.1)\n",
    "plt.title('HRD of Hippacros Catalogue corrected')\n",
    "plt.xlabel('B-V')\n",
    "plt.ylabel('Hp')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One trait now looks more prominent, and that is the suspiciously high number of stars with $ B-V = 0 $. This is called an artefact, and it is a result of some bad data or sloppy reduction. Since the author of this reduction is quite renowned in the community, the issue most likely lies with bad data for these stars. For the rest of the exercise, you may filter-out star with $B-V = 0$.\n",
    "\n",
    "Now try plotting the HRD in terms of the intrinsic absolute magnitude, $ M_{Hp} $ (you should get something similar in shape to [this](https://www.cosmos.esa.int/documents/532822/573165/f3_5_005.pdf/90951100-6586-4ba7-a042-784f5845a279)). **Try also with the other colour $V-I$**. Remember that the absolute magnitude can be calculated by the formula:\n",
    "\n",
    "$$\n",
    "M_\\mathrm{Hp} = m_\\mathrm{Hp} - 5 \\ \\mathrm{log}_\\mathrm{10}\\Big(\\frac{d}{pc}\\Big) + 5\n",
    "$$\n",
    "\n",
    "This plot has a bit of an issue where there are many points plotted ontop of each other. Instead, see if you can plot the number density in a 2D histogram of of the following ways **10 points**\n",
    "\n",
    "* bin the data in steps of 0.05 mag in $M_{Hp}$ and 0.01 mag in $B-V$, with the number density in the color bar. You might find the function `scipy.stats.binned_statistic_2d()` useful.\n",
    "* Utilise the `hexbin` routine with an adequate choice of binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_V_limit_np_new= np.where(B_V_limit_np==0, np.nan, B_V_limit_np)\n",
    "V_I_limit_np_new= np.where(V_I_limit_np==0, np.nan, V_I_limit_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MHP'] = (data['Hp'])-(5*np.log(data['Distance (pc)']))+5\n",
    "MHP = np.asarray(data['MHP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = B_V_limit_np_new, y = MHP, s = 0.1)\n",
    "plt.title('HRD of Hippacros Catalogue corrected with B-V')\n",
    "plt.xlabel('B-V')\n",
    "plt.ylabel('absolute Magnitude')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x = V_I_limit_np_new, y = MHP, s = 0.1)\n",
    "plt.title('HRD of Hippacros Catalogue corrected with V-I')\n",
    "plt.xlabel('I-V')\n",
    "plt.ylabel('absolute Magnitude')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_V_new= np.where(B_V_limit_np_new==np.nan, 0, B_V_limit_np_new)\n",
    "MHP_new= np.where(MHP==np.nan, 0, MHP)\n",
    "z=np.arange(len(MHP_new))\n",
    "\n",
    "#stats.binned_statistic_2d(B_V_new, MHP_new, values = z, statistic ='count', bins = [0.01, 0.05])\n",
    "#binned statistic 2d ist not wokring properly, but we don't now whu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very broadly we say that the features observed here are the main sequence (diagonal feature) and red clump (roughly spherical feature). Fit a line to the main sequence. Approximate the percentage of stars in the main sequence compared to the percentage in the red clump. **10 points**\n",
    "\n",
    "> *Note:* This will not add up to 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = B_V_limit_np_new, y = MHP, s = 0.1)\n",
    "plt.title('HRD of Hippacros Catalogue corrected with B-V')\n",
    "plt.xlabel('B-V')\n",
    "plt.ylabel('absolute Magnitude')\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, -y, 1))(np.unique(x)),color='red')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical\n",
    "\n",
    "For theorists it is more illuminating to plot diagrams of intrinsic properties. This makes it easier to relate the observations to population synthesis or evolutionary models. Getting to these intrinsic stellar properties from our observations is typically quite involved, requiring some (not so trivial) estimate of the dust extinction and attenuation along the line of sight. We will assume there is no dust extinction for a very basic example.\n",
    "\n",
    "A star's colour can be used to calculate its effective (surface) temperature. Disregarding any reddenning, we can estimate it using the formula ([Ballesteros 2012](https://arxiv.org/pdf/1201.1809.pdf)):\n",
    "\n",
    "$$\n",
    "T_\\mathrm{eff} = 4600 \\ \\Big( \\frac{1}{0.92(B-V)+0.62} + \\frac{1}{0.92(B-V)+1.7} \\Big) \\ \\mathrm{K}\n",
    "$$\n",
    "\n",
    "Luminosity is the intrinsic analog of magnitude. To put it in solar units, we need to use the absolute bolometric magnitude of the Sun ($M_\\odot=4.74$) with the observed star's bolometric magnitude and extinction. Since our absolute magnitudes are in the V band, we will have to add a bolometric correction ($BC_\\mathrm{Hp}$) to convert to the bolometric magnitude. Thus we have too many unknowns for our to make an accurate approximation. Current missions (ie. *Gaia*) get around this by utilising extremely-randomised trees, a regression method beyond the scope of this simple introduction. We will **not** use regression, but instead use an extremely-simplified approach where we neglect extinction and interpolate the bolometric correction (from `BC_hipparcos.csv`; [Masana et al. 2008](https://arxiv.org/pdf/astro-ph/0601049.pdf)) to get the formula:\n",
    "\n",
    "$$\n",
    "-2.5 \\ \\mathrm{log_{10}} \\Big( \\frac{L}{L_\\odot} \\Big) = M_\\mathrm{Hp} + BC_\\mathrm{Hp}(T_\\mathrm{eff}) - M_\\odot\n",
    "$$\n",
    "\n",
    "Finally, the stellar radii can be calculated using the standard blackbody relation:\n",
    "\n",
    "$$\n",
    "L = \\Big( \\frac{R}{R_\\odot} \\Big)^2 \\Big( \\frac{T_\\mathrm{eff}}{T_{\\mathrm{eff},\\odot}} \\Big)^4 \\ L_\\odot\n",
    "$$\n",
    "\n",
    "It might also help you to know the solar units: $L_\\odot=3.828*10^{26} \\ \\mathrm{W}$, $R_\\odot=6.956*10^8 \\ \\mathrm{m}$, $T_\\odot=5780 \\ \\mathrm{K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a spline fit of the bolometric correction data. Correctly interpolate the BC and utilise the equations to create a 2D histogram (or hexbin plot) of $L$ as a function of $T_\\mathrm{eff}$. **10 points**\n",
    "\n",
    "> *hint:* Use units of $L_\\odot$ and K.\n",
    "\n",
    "> *hint:* Technically this fit is only valid for $T_\\mathrm{eff} \\in [3300, 8000]$ K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = pd.read_csv('BC_hipparcos.csv', sep=',')\n",
    "BC = np.asarray(data_2['BC'])\n",
    "T = np.asarray(data_2['# Teff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = interpolate.interp1d(T,BC)\n",
    "T_new = np.arange(4000,8000,4000/len(MHP))\n",
    "BC_new = f(T_new)\n",
    "data['BC_new']=BC_new\n",
    "data['T_new']=T_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MHP_new= np.where(MHP==np.nan, 0, MHP)\n",
    "data['MHP_new']=MHP_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Luminosity'] = 10**((0.4*(4.74-data['MHP_new']-data['BC_new'])))\n",
    "data['Luminosity'].replace([np.inf, np.nan, -np.inf], 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.asarray(data['Luminosity'])\n",
    "T = np.asarray(data['T_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(T,L)\n",
    "#plt.plot(np.unique(T), np.poly1d(np.polyfit(T, L, 1))(np.unique(T)),color='red')\n",
    "plt.xlabel('T(K)')\n",
    "plt.ylabel('L')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to perform a linear fit to the main sequence, accounting for error. Estimate the error in the fit. **10 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.unique(T), np.poly1d(np.polyfit(T, L, 1))(np.unique(T)),color='red')\n",
    "plt.xlabel('T(K)')\n",
    "plt.ylabel('L')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We interpolated the BC values and calculated the Lumonisity. For the hist2d plot we could not find a good solution. The Main Error here is a lot of nan in the data. But the linear fit without the hist2d plot gave a good result as expected. The Luminosity arises if the temperature does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out how to isolate the stars on the main sequence. What is the probability for a star to be on the main sequence? Use the bootstrapping technique to determine the constrain the error in this approximation. **10 points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maybe with the proper motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_PM_RA = data['PM_RA']>10\n",
    "filter_PM_DEC = data['PM_DEC']>10\n",
    "filter_PM= filter_PM_RA*filter_PM_DEC\n",
    "B_V_last = B_V_limit_np_new*filter_PM\n",
    "MHP_last = MHP*filter_PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = B_V_last, y = MHP_last, s = 0.1)\n",
    "plt.title('HRD of Hippacros Catalogue corrected with B-V')\n",
    "plt.xlabel('B-V')\n",
    "plt.ylabel('absolute Magnitude')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### could not isolate the main sequence with the proper motion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
